{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, make_scorer\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgm\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import scipy\n",
    "from scipy.cluster import hierarchy as hc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used functionality \n",
    "\n",
    "def mad(x,y): return sum(abs(x-y))/len(x)\n",
    "\n",
    "def handle_multi_string_columns(df, column, single_strings):\n",
    "    '''\n",
    "    Replaces column whose fields contain several strings with new columns. Each\n",
    "    new column will then represent a single string\n",
    "    \n",
    "    INPUT:\n",
    "    df - the pandas dataframe you want to search\n",
    "    column - the column name you want to look through\n",
    "    single_strings - a list of strings you want to search for in each row of df[col]\n",
    "\n",
    "    OUTPUT:\n",
    "    new_df - The dataframe without the multi-string column but with the newly created columns\n",
    "    col_dict - Dictionary translating names of the new columns to their corresponding string\n",
    "    '''\n",
    "    \n",
    "    #collects new columns of indicating if a certain index refers to a string \n",
    "    new_columns = dict()\n",
    "    \n",
    "    #dict column name -> string name\n",
    "    col_dict = dict()\n",
    "    \n",
    "    #loop through list of strings\n",
    "    counter = 0\n",
    "    for string in single_strings:\n",
    "        bool_list = []\n",
    "        #loop through rows\n",
    "        for idx in range(df.shape[0]):\n",
    "            #if the ed type is in the row set to True\n",
    "            if string in str(df[column][idx]):\n",
    "                bool_list.append(1)\n",
    "            else:\n",
    "                bool_list.append(0)\n",
    "        col_name = column + \"_\" + str(counter)\n",
    "        new_columns[col_name] = bool_list\n",
    "        col_dict[col_name] = string\n",
    "        counter = counter + 1\n",
    "    \n",
    "    new_df = df.drop(column,axis=1)\n",
    "    \n",
    "    new_df = pd.concat([new_df, pd.DataFrame(data=new_columns, index = df.index, dtype=int)], axis=1)\n",
    "    \n",
    "    return new_df, col_dict\n",
    "\n",
    "\n",
    "def rf_feat_importance(m, df):\n",
    "    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}).sort_values('imp', ascending=False)\n",
    "\n",
    "def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n",
    "\n",
    "def mean_absolute_error_2(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, np.round(y_pred))\n",
    "\n",
    "\n",
    "def classification(y_train, y_test_preds):\n",
    "    y_train_min = np.min(y_train)\n",
    "    y_train_max = np.max(y_train)\n",
    "    total_distance = y_train_max - y_train_min\n",
    "    point_0 = y_train_min + total_distance * np.sum(y_train == 0) / len(y_train)\n",
    "    point_1 = point_0 + total_distance * np.sum(y_train == 1) / len(y_train)\n",
    "    point_2 = point_1 + total_distance * np.sum(y_train == 2) / len(y_train)\n",
    "    point_3 = point_2 + total_distance * np.sum(y_train == 3) / len(y_train)\n",
    "    point_4 = y_train_max\n",
    "    \n",
    "    for i in range(len(y_test_preds)):\n",
    "        if y_test_preds[i] <= point_0:\n",
    "            y_test_preds[i] = 0\n",
    "        elif y_test_preds[i] <= point_1:\n",
    "            y_test_preds[i] = 1\n",
    "        elif y_test_preds[i] <= point_2:\n",
    "            y_test_preds[i] = 2\n",
    "        elif y_test_preds[i] <= point_3:\n",
    "            y_test_preds[i] = 3\n",
    "        elif y_test_preds[i] <= point_4:\n",
    "            y_test_preds[i] = 4\n",
    "        else:\n",
    "            print(\"An error occurred!\")\n",
    "            break\n",
    "    \n",
    "        return y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('data/survey_results_public.csv', low_memory=False)\n",
    "schema = pd.read_csv('data/survey_results_schema.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.dropna(subset=['JobSat'], axis=0)\n",
    "df = df[df.Employment.isin(['Employed full-time', 'Employed part-time'])]\n",
    "df = df[df.Student == 'No']\n",
    "df = df.drop(['CareerSat','Respondent','ResumeUpdate','CurrencySymbol','CurrencyDesc','CompTotal','SurveyEase','SurveyLength','SONewContent','WelcomeChange','SOComm','EntTeams','SOVisit1st',\n",
    "                 'SOVisitFreq', 'SOVisitTo', 'SOFindAnswer', 'SOTimeSaved', 'SOHowMuchTime', 'SOAccount','SOPartFreq', 'SOJobs', 'LanguageWorkedWith', 'LanguageDesireNextYear',\n",
    "                 'DatabaseWorkedWith', 'DatabaseDesireNextYear', 'PlatformWorkedWith', 'PlatformDesireNextYear', 'WebFrameWorkedWith', 'WebFrameDesireNextYear', 'MiscTechWorkedWith', \n",
    "                 'MiscTechDesireNextYear', 'DevEnviron','Student','Ethnicity','JobSeek','MgrIdiot'], axis=1)\n",
    "df = df.dropna(thresh=45)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EduOther_strings = ['Taken an online course in programming or software development (e.g. a MOOC)','Participated in a fulltime developer training program or bootcamp',\n",
    "                    'Taken a part-time in-person course in programming or software development','Completed an industry certification program (e.g. MCPD)',\n",
    "                    'Received on-the-job training in software development','Taught yourself a new language, framework, or tool without taking a formal course',\n",
    "                    'Participated in online coding competitions (e.g. HackerRank, CodeChef, TopCoder)','Participated in a hackathon','Contributed to open source software',\n",
    "                    'None of these']\n",
    "\n",
    "DevType_strings = ['Academic researcher','Data or business analyst','Data scientist or machine learning specialist','Database administrator','Designer','Developer, back-end',\n",
    "                   'Developer, desktop or enterprise applications','Developer, embedded applications or devices','Developer, front-end','Developer, full-stack',\n",
    "                   'Developer, game or graphics','Developer, mobile','Developer, QA or test','DevOps specialist','Educator','Engineer, data','Engineer, site reliability',\n",
    "                   'Engineering manager','Marketing or sales professional','Product manager','Scientist',\"Senior Executive (C-Suite, VP, etc.)\", 'Student', 'System administrator']\n",
    "\n",
    "LastInt_strings = ['Write any code','Write code by hand (e.g., on a whiteboard)','Complete a take-home project','Solve a brain-teaser style puzzle','Interview with people in peer roles',\n",
    "                   'Interview with people in senior / management roles']\n",
    "\n",
    "WorkChallenge_strings = ['Distracting work environment','Being tasked with non-development work','Meetings','Time spent commuting','Not enough people for the workload',\n",
    "                         'Toxic work environment','Inadequate access to necessary tools','Lack of support from management','Non-work commitments (parenting, school work, hobbies, etc.)']\n",
    "\n",
    "JobFactors_strings = ['Diversity of the company or organization',\"Languages, frameworks, and other technologies I'd be working with\",\"Industry that I'd be working in\",\n",
    "                      'How widely used or impactful my work output would be',\"Specific department or team I'd be working on\",'Flex time or a flexible schedule',\n",
    "                      'Remote work options','Financial performance or funding status of the company or organization','Office environment or company culture',\n",
    "                      'Opportunities for professional development']\n",
    "\n",
    "df, dict_EduOther = handle_multi_string_columns(df,\"EduOther\",EduOther_strings)\n",
    "df, dict_DevType = handle_multi_string_columns(df,\"DevType\",DevType_strings)\n",
    "df, dict_LastInt = handle_multi_string_columns(df,\"LastInt\",LastInt_strings)\n",
    "df, dict_WorkChallenge = handle_multi_string_columns(df,\"WorkChallenge\",WorkChallenge_strings)\n",
    "df, dict_JobFactors = handle_multi_string_columns(df,\"JobFactors\",JobFactors_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,c in df.items():\n",
    "        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n",
    "\n",
    "df.JobSat.cat.set_categories(['Very dissatisfied','Slightly dissatisfied','Neither satisfied nor dissatisfied','Slightly satisfied', 'Very satisfied'], ordered=True, inplace=True)\n",
    "df.CareerSat.cat.set_categories(['Very dissatisfied','Slightly dissatisfied','Neither satisfied nor dissatisfied','Slightly satisfied', 'Very satisfied'], ordered=True, inplace=True)\n",
    "#df.JobSeek.cat.set_categories(['I am not interested in new job opportunities','I’m not actively looking, but I am open to new opportunities','I am actively looking for a job'], ordered=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indicator = df.isnull().astype(int).add_suffix('_nan')\n",
    "df = pd.concat([df, df_indicator], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = df.select_dtypes(['category']).columns\n",
    "df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "\n",
    "fill_mean = lambda col: col.fillna(col.mean())\n",
    "# Fill the mean\n",
    "df = df.apply(fill_mean, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns with all NaN values\n",
    "df = df.dropna(how='all', axis=0)\n",
    "df = df.dropna(how='all', axis=1)\n",
    "#delete columns that add up to 0\n",
    "df = df.loc[:, (df != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['JobSat'], axis=1)\n",
    "y = df['JobSat']\n",
    "\n",
    "X_train, X_test_valid, y_train, y_test_valid = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_test_valid, y_test_valid, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Should set group for ranking task",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-8356eeb7af22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMRanker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/source/JobSatisfaction/.env/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_group, eval_metric, eval_at, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0;31m# check group data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Should set group for ranking task\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval_set\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Should set group for ranking task"
     ]
    }
   ],
   "source": [
    "model = lgm.LGBMRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiplier: 1.42\n",
      "Scalar: -1.0\n",
      "The rsquared on the training data was 0.27753058024364674.  The rsquared on the test data was 0.16313177958100977.\n",
      "The mad on the training data was 0.7786937048333471.  The mad on the test data was 0.8740520936366634.\n"
     ]
    }
   ],
   "source": [
    "#Predict using your model\n",
    "y_test_preds = model.predict(X_test)\n",
    "y_train_preds =model.predict(X_train)\n",
    "#Score using your model\n",
    "test_r2 = r2_score(y_test, y_test_preds)\n",
    "train_r2 = r2_score(y_train, y_train_preds)\n",
    "# train_mad = mad(y_train, classification(y_train, y_train_preds))\n",
    "# test_mad = mad(y_test, classification(y_train, y_test_preds))\n",
    "\n",
    "correct = lambda x, multi, add: multi * x + add\n",
    "corrector = 0 \n",
    "train_mad = 1\n",
    "for add in range(-100, 100):\n",
    "    for multi in range (0, 200):\n",
    "        new_mad = mad(y_train, np.round(correct(y_train_preds, multi/100, add/100)))\n",
    "        if new_mad < train_mad:\n",
    "            train_mad = new_mad\n",
    "            multi_final = multi / 100\n",
    "            add_final = add / 100\n",
    "\n",
    "test_mad = mad(y_test, np.round(correct(y_test_preds, multi_final,add_final)))\n",
    "\n",
    "print(\"Multiplier: {}\".format(multi_final))\n",
    "print(\"Scalar: {}\".format(add_final))\n",
    "print(\"The rsquared on the training data was {}.  The rsquared on the test data was {}.\".format(train_r2, test_r2))\n",
    "print(\"The mad on the training data was {}.  The mad on the test data was {}.\".format(train_mad, test_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 22, 1499, 5979, 1599]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.sum(np.round(y_test_preds + corrector) == 0) , np.sum(np.round(y_test_preds + corrector) == 1), np.sum(np.round(y_test_preds + corrector) == 2), np.sum(np.round(y_test_preds + corrector) == 3), np.sum(np.round(y_test_preds + corrector) == 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_g_0 = y.apply(lambda k: 0 if k<=0 else 1) # probability greater 0\n",
    "y_g_1 = y.apply(lambda k: 0 if k<=1 else 1) # probability greater 1\n",
    "y_g_2 = y.apply(lambda k: 0 if k<=2 else 1) # probability greater 2\n",
    "y_g_3 = y.apply(lambda k: 0 if k<=3 else 1) # probability greater 3\n",
    "\n",
    "y_g_0_train = y_g_0.loc[y_train.index]\n",
    "y_g_1_train = y_g_1.loc[y_train.index]\n",
    "y_g_2_train = y_g_2.loc[y_train.index]\n",
    "y_g_3_train = y_g_3.loc[y_train.index]\n",
    "\n",
    "y_g_0_test = y_g_0.loc[y_test.index]\n",
    "y_g_1_test = y_g_1.loc[y_test.index]\n",
    "y_g_2_test = y_g_2.loc[y_test.index]\n",
    "y_g_3_test = y_g_3.loc[y_test.index]\n",
    "\n",
    "y_g_0_valid = y_g_0.loc[y_valid.index]\n",
    "y_g_1_valid = y_g_1.loc[y_valid.index]\n",
    "y_g_2_valid = y_g_2.loc[y_valid.index]\n",
    "y_g_3_valid = y_g_3.loc[y_valid.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rsquared on the training data was 0.22292569493738645.  The rsquared on the test data was 0.023159726953167592.\n",
      "The mad on the training data was 0.06402330118429368.  The mad on the test data was 0.06726013847675569.\n"
     ]
    }
   ],
   "source": [
    "model_0 = lgm.LGBMRegressor()\n",
    "model_0.fit(X_train, y_g_0_train)\n",
    "\n",
    "#Predict using your model\n",
    "y_g_0_train_preds =model_0.predict(X_train)\n",
    "y_g_0_test_preds = model_0.predict(X_test)\n",
    "\n",
    "#Score using your model\n",
    "train_0_score = r2_score(y_g_0_train, y_g_0_train_preds)\n",
    "test_0_score = r2_score(y_g_0_test, y_g_0_test_preds)\n",
    "\n",
    "train_0_mad = mad(y_g_0_train, np.round(y_g_0_train_preds))\n",
    "test_0_mad = mad(y_g_0_test, np.round(y_g_0_test_preds))\n",
    "\n",
    "print(\"The rsquared on the training data was {}.  The rsquared on the test data was {}.\".format(train_0_score, test_0_score))\n",
    "print(\"The mad on the training data was {}.  The mad on the test data was {}.\".format(train_0_mad, test_0_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rsquared on the training data was 0.22847372696399826.  The rsquared on the test data was 0.09556611754870326.\n",
      "The mad on the training data was 0.18569505124611876.  The mad on the test data was 0.22035388504231235.\n"
     ]
    }
   ],
   "source": [
    "model_1 = lgm.LGBMRegressor()\n",
    "model_1.fit(X_train, y_g_1_train)\n",
    "\n",
    "#Predict using your model\n",
    "y_g_1_train_preds =model_1.predict(X_train)\n",
    "y_g_1_test_preds = model_1.predict(X_test)\n",
    "\n",
    "#Score using your model\n",
    "train_1_score = r2_score(y_g_1_train, y_g_1_train_preds)\n",
    "test_1_score = r2_score(y_g_1_test, y_g_1_test_preds)\n",
    "\n",
    "train_1_mad = mad(y_g_1_train, np.round(y_g_1_train_preds))\n",
    "test_1_mad = mad(y_g_1_test, np.round(y_g_1_test_preds))\n",
    "\n",
    "print(\"The rsquared on the training data was {}.  The rsquared on the test data was {}.\".format(train_1_score, test_1_score))\n",
    "print(\"The mad on the training data was {}.  The mad on the test data was {}.\".format(train_1_mad, test_1_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rsquared on the training data was 0.24782716687712625.  The rsquared on the test data was 0.13208735024867757.\n",
      "The mad on the training data was 0.2404308520869398.  The mad on the test data was 0.2919002088141554.\n"
     ]
    }
   ],
   "source": [
    "model_2 = lgm.LGBMRegressor()\n",
    "model_2.fit(X_train, y_g_2_train)\n",
    "\n",
    "#Predict using your model\n",
    "y_g_2_train_preds =model_2.predict(X_train)\n",
    "y_g_2_test_preds = model_2.predict(X_test)\n",
    "\n",
    "#Score using your model\n",
    "train_2_score = r2_score(y_g_2_train, y_g_2_train_preds)\n",
    "test_2_score = r2_score(y_g_2_test, y_g_2_test_preds)\n",
    "\n",
    "train_2_mad = mad(y_g_2_train, np.round(y_g_2_train_preds))\n",
    "test_2_mad = mad(y_g_2_test, np.round(y_g_2_test_preds))\n",
    "\n",
    "print(\"The rsquared on the training data was {}.  The rsquared on the test data was {}.\".format(train_2_score, test_2_score))\n",
    "print(\"The mad on the training data was {}.  The mad on the test data was {}.\".format(train_2_mad, test_2_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rsquared on the training data was 0.2511245086581584.  The rsquared on the test data was 0.12992872439370173.\n",
      "The mad on the training data was 0.24103536394361552.  The mad on the test data was 0.28552588196505113.\n"
     ]
    }
   ],
   "source": [
    "model_3 = lgm.LGBMRegressor()\n",
    "model_3.fit(X_train, y_g_3_train)\n",
    "\n",
    "#Predict using your model\n",
    "y_g_3_train_preds =model_3.predict(X_train)\n",
    "y_g_3_test_preds = model_3.predict(X_test)\n",
    "\n",
    "#Score using your model\n",
    "train_3_score = r2_score(y_g_3_train, y_g_3_train_preds)\n",
    "test_3_score = r2_score(y_g_3_test, y_g_3_test_preds)\n",
    "\n",
    "train_3_mad = mad(y_g_3_train, np.round(y_g_3_train_preds))\n",
    "test_3_mad = mad(y_g_3_test, np.round(y_g_3_test_preds))\n",
    "\n",
    "print(\"The rsquared on the training data was {}.  The rsquared on the test data was {}.\".format(train_3_score, test_3_score))\n",
    "print(\"The mad on the training data was {}.  The mad on the test data was {}.\".format(train_3_mad, test_3_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_g_0_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_prob(y_g_0_preds,y_g_1_preds,y_g_2_preds,y_g_3_preds):\n",
    "    y_0 = 1 - y_g_0_preds\n",
    "    y_1 = 1 - y_g_1_preds - y_0\n",
    "    y_2 = 1 - y_g_2_preds - y_0 - y_1\n",
    "    y_3 = 1 - y_g_3_preds - y_0 - y_1 - y_2\n",
    "    y_4 = 1 - y_0 - y_1 - y_2 - y_3\n",
    "    #     y_4 = y_g_3_preds\n",
    "    #     y_3 = y_g_2_preds - y_4\n",
    "    #     y_2 = y_g_1_preds - y_4 - y_3\n",
    "    #     y_1 = y_g_0_preds - y_4 - y_3 - y_2 \n",
    "    #     y_0 = 1 - y_1 - y_2 - y_3 - y_4\n",
    "    \n",
    "    \n",
    "#     adjust = np.vectorize(lambda x: np.maximum(x,0))\n",
    "#     y_0 = adjust(y_0)\n",
    "#     y_1 = adjust(y_1)\n",
    "#     y_2 = adjust(y_2)\n",
    "#     y_3 = adjust(y_3)\n",
    "#     y_4 = adjust(y_4)\n",
    "    for i in range(len(y_0)):\n",
    "        if y_0[i] < 0:\n",
    "            y_1[i] = y_1[i] - y_0[i]\n",
    "            y_2[i] = y_2[i] - y_0[i]\n",
    "            y_3[i] = y_3[i] - y_0[i]\n",
    "            y_4[i] = y_4[i] - y_0[i]\n",
    "            y_0[i] = 0\n",
    "        if y_1[i] < 0:\n",
    "            y_0[i] = y_0[i] - y_1[i]\n",
    "            y_2[i] = y_2[i] - y_1[i]\n",
    "            y_3[i] = y_3[i] - y_1[i]\n",
    "            y_4[i] = y_4[i] - y_1[i]\n",
    "            y_1[i] = 0\n",
    "        if y_2[i] < 0:\n",
    "            y_1[i] = y_1[i] - y_2[i]\n",
    "            y_0[i] = y_0[i] - y_2[i]\n",
    "            y_3[i] = y_3[i] - y_2[i]\n",
    "            y_4[i] = y_4[i] - y_2[i]\n",
    "            y_2[i] = 0\n",
    "        if y_3[i] < 0:\n",
    "            y_1[i] = y_1[i] - y_3[i]\n",
    "            y_2[i] = y_2[i] - y_3[i]\n",
    "            y_0[i] = y_0[i] - y_3[i]\n",
    "            y_4[i] = y_4[i] - y_3[i]\n",
    "            y_3[i] = 0\n",
    "        if y_4[i] < 0:\n",
    "            y_1[i] = y_1[i] - y_4[i]\n",
    "            y_2[i] = y_2[i] - y_4[i]\n",
    "            y_3[i] = y_3[i] - y_4[i]\n",
    "            y_0[i] = y_0[i] - y_4[i]\n",
    "            y_4[i] = 0\n",
    "            \n",
    "\n",
    "            \n",
    "    y_normalize = y_0 + y_1 + y_2 + y_3 + y_4\n",
    "    for i in range(len(y_0)):\n",
    "        y_0[i] = y_0[i] / y_normalize[i]\n",
    "        y_1[i] = y_1[i] / y_normalize[i]\n",
    "        y_2[i] = y_2[i] / y_normalize[i]\n",
    "        y_3[i] = y_3[i] / y_normalize[i]\n",
    "        y_4[i] = y_4[i] / y_normalize[i]\n",
    "    return y_0, y_1, y_2, y_3, y_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0_test, y_1_test, y_2_test, y_3_test, y_4_test = categorical_prob(y_g_0_test_preds, y_g_1_test_preds, y_g_2_test_preds, y_g_3_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8763600395647874"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = np.empty_like(y_0_test)\n",
    "\n",
    "expected_values = y_1_test * 1 + y_2_test * 2 + y_3_test * 3 + y_4_test * 4\n",
    "\n",
    "for i in range(len(prediction)):\n",
    "    heighest = y_0_test[i]\n",
    "    prediction[i]=0\n",
    "    if heighest < y_1_test[i]:\n",
    "        heighest = y_1_test[i]\n",
    "        prediction[i]=1\n",
    "    if heighest < y_2_test[i]:\n",
    "        heighest = y_2_test[i]\n",
    "        prediction[i]=2\n",
    "    if heighest < y_3_test[i]:\n",
    "        heighest = y_3_test[i]\n",
    "        prediction[i]=3\n",
    "    if heighest < y_4_test[i]:\n",
    "        heighest = y_4_test[i]\n",
    "        prediction[i]=4\n",
    "    if heighest < 0.50:\n",
    "        prediction[i] = np.round(expected_values[i])\n",
    "\n",
    "        \n",
    "        \n",
    "mad(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[614, 1452, 954, 3213, 2866]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.sum(y_test == 0) , np.sum(y_test == 1), np.sum(y_test == 2), np.sum(y_test == 3), np.sum(y_test == 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 114, 2695, 4793, 1495]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.sum(prediction == 0) , np.sum(prediction == 1), np.sum(prediction == 2), np.sum(prediction == 3), np.sum(prediction == 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
